#################################

import boto3
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_aws import ChatBedrock, AmazonKnowledgeBasesRetriever
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
import streamlit as st
from operator import itemgetter
from langchain_core.prompts import MessagesPlaceholder
import botocore.exceptions  # Importamos excepciones espec√≠ficas de Boto3
from typing import List, Dict
from pydantic import BaseModel  ##Importante esto a veces no es compatible
from langchain.schema import Document
from langchain.schema.runnable import RunnableLambda
from langchain_community.chat_message_histories import DynamoDBChatMessageHistory
import requests

#from langchain_openai.chat_models import ChatOpenAI

from collections import defaultdict
from langchain.schema import Document  # Aseg√∫rate de importar Document si es necesario
from dotenv import load_dotenv
from langsmith import traceable
from langsmith import Client

#from streamlit_feedback import streamlit_feedback

from langsmith.run_helpers import get_current_run_tree

from streamlit_feedback import streamlit_feedback

from langchain.callbacks import collect_runs



from langsmith.run_helpers import get_current_run_tree


# Cargar variables de entorno
load_dotenv()



client = Client()


class Citation(BaseModel):
    page_content: str
    metadata: Dict

def extract_citations(response: List[Dict]) -> List[Citation]:
    return [Citation(page_content=doc.page_content, metadata=doc.metadata ) for doc in response]

# ------------------------------------------------------
# S3 Presigned URL, esto permite realizar descargar del documento

def create_presigned_url(bucket_name: str, object_name: str, expiration: int = 300) -> str:
    """Generate a presigned URL to share an S3 object"""
    s3_client = boto3.client('s3')
    try:
        response = s3_client.generate_presigned_url('get_object',
                                                    Params={'Bucket': bucket_name,
                                                            'Key': object_name},
                                                    ExpiresIn=expiration)
    except NoCredentialsError:
        st.error("AWS credentials not available")
        return ""
    return response

def parse_s3_uri(uri: str) -> tuple:
    """Parse S3 URI to extract bucket and key"""
    parts = uri.replace("s3://", "").split("/")
    bucket = parts[0]
    key = "/".join(parts[1:])
    return bucket, key


# Configuraci√≥n de Bedrock
bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-east-1")
#TODO: model ID

model_id = "anthropic.claude-3-haiku-20240307-v1:0"
# An error occurred (ThrottlingException) when calling the InvokeModelWithResponseStream operation (reached max retries: 4): Too many tokens, please wait before trying again.
#model_id="anthropic.claude-3-sonnet-20240229-v1:0" ##Por ejemplo este es m√°s tardado
#model_id="anthropic.claude-3-5-haiku-20241022-v1:0" ##3.5 haiku no esta disponible on demand
#model_id= "anthropic.claude-3-5-sonnet-20241022-v2:0"      #Claude 3.5 Sonnet v2, mismo caso anterior
#model_id= "anthropic.claude-3-5-sonnet-20240620-v1:0"  ##Claude 3.5 Sonnet v1, si responde m√°s tardado..., si funciona en este entorno
# importante encontrar cuales son los otros limites por modelo


model_kwargs = {
    "max_tokens": 4096,
    "temperature": 0.0,
    "top_k": 250,
    "top_p": 1,
    "stop_sequences": ["\n\nHuman"],
}


#inference_profile = "us.meta.llama3-2-3b-instruct-v1:0"

inference_profile1="us.anthropic.claude-3-5-haiku-20241022-v1:0"
inference_profile="us.anthropic.claude-3-haiku-20240307-v1:0"

# us.meta.llama3-2-11b-instruct-v1:0

model_kwargs2 = {
    "max_tokens": 4096,
    "temperature": 0.0,
}
# Generador de respuesta ChatBedrock:
llmClaude = ChatBedrock(
    client=bedrock_runtime,
    model_id=inference_profile,
    model_kwargs=model_kwargs,
)




# Funci√≥n que genera la configuraci√≥n completa del retriever de la base de conocimientos
def generar_configuracion_retriever_new():
    # Verificar si `st.session_state.centros_costos` existe y tiene datos
    if "centros_costos" in st.session_state and st.session_state.centros_costos:
        # Obtener los c√≥digos de centros de costos activos (donde "ACTIVO" == "Y")
        activas = [cc["CODIGO"] for cc in st.session_state.centros_costos if cc["ACTIVO"] == "Y"]
    else:
        activas = []  # Si no hay datos, la lista queda vac√≠a

        
    #print(activas)

    # Construir la configuraci√≥n base
    config = {
        "vectorSearchConfiguration": {
            "numberOfResults": 100  # M√°ximo 100
        }
    }

    # Aplicar filtro solo si hay centros de costos activos
    if activas:
        config["vectorSearchConfiguration"]["filter"] = {
            "in": {
                "key": "codigo_area",
                "value": activas
            }
        }

    #print(config)

    return config




# Funci√≥n para recuperar y depurar el contexto
def obtener_contexto(inputs, retriever):
    """
    Funci√≥n que recibe un retriever como par√°metro para obtener los documentos relevantes.
    """
    question = inputs["question"]  # Extraer la pregunta
    documentos = retriever.invoke(question)  # Obtener documentos relevantes

    # Diccionario para agrupar contenido por 'identificador_proceso'
    procesos_agrupados = defaultdict(list)

    # Iterar sobre los documentos recuperados
    for doc in documentos:
        # Extraer los metadatos y el identificador
        source_metadata = doc.metadata.get('source_metadata', {})
        identificador = source_metadata.get('identificador_proceso', 'Sin ID')

        # Agrupar el contenido del documento bajo el mismo identificador
        procesos_agrupados[identificador].append(doc.page_content)

    # Crear una nueva lista de Documentos con contenido concatenado por identificador_proceso
    documentos_concatenados = []
    for identificador, contenidos in procesos_agrupados.items():
        # Concatenar los contenidos
        contenido_concatenado = "\n".join(contenidos)

        # Tomar los metadatos y el score del primer documento con este identificador
        doc_base = next(
            (doc for doc in documentos if doc.metadata.get('source_metadata', {}).get('identificador_proceso') == identificador),
            None
        )

        if doc_base:
            metadatos_base = doc_base.metadata  # Metadatos del primer documento
            score_base = doc_base.metadata.get('score', 0)  # Score del primer documento
        else:
            metadatos_base = {}
            score_base = 0

        # Incluir el score en los metadatos base
        metadatos_base['score'] = score_base

        # Crear un nuevo objeto Document con el contenido concatenado
        documento_concatenado = Document(
            metadata=metadatos_base,
            page_content=contenido_concatenado
        )

        # Agregar el documento a la lista final
        documentos_concatenados.append(documento_concatenado)

    return documentos_concatenados  # Devolver los documentos concatenados


# Crear el pipeline con depuraci√≥n, ahora requiere un retriever al ser llamado
def crear_context_pipeline(retriever):
    return RunnableLambda(lambda inputs: obtener_contexto(inputs, retriever))


# Nombre de la tabla en DynamoDB
table_name = "SessionTable"


# Funci√≥n para crear el prompt din√°mico
def create_prompt_template(system_prompt):
    return ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}")
        ]
    )

# Chain con historial de mensajes
def create_chain_with_history(retriever,system_prompt):
    """
    Funci√≥n que crea el chain con historial de mensajes y requiere un retriever.
    """
    prompt = create_prompt_template(system_prompt)

    chain = (
        RunnableParallel({
            "context": crear_context_pipeline(retriever),  # Usamos la funci√≥n que crea el pipeline con retriever
            "question": itemgetter("question"),
            "history": itemgetter("history"),
        })
        .assign(response=prompt | llmClaude | StrOutputParser())
        .pick(["response", "context"])
    )

    history_old = StreamlitChatMessageHistory(key="chat_messages")

    return RunnableWithMessageHistory(
        chain,
        lambda session_id: DynamoDBChatMessageHistory(
            table_name=table_name, session_id=session_id
        ),
        input_messages_key="question",
        history_messages_key="history",
        output_messages_key="response",
    )


# Funci√≥n para manejar errores de Bedrock, aqui se busca que se vuelve a repetir la consulta
def handle_error(error):
    st.error("Ocurri√≥ un problema. Por favor, repite tu consulta.")
    st.write("Detalles t√©cnicos (para depuraci√≥n):")
    st.code(str(error))  # Mostrar los detalles del error para prop√≥sitos de depuraci√≥n

###################################################################################################

def invoke_with_retriesnew(chain, prompt, history, config, max_retries=10):
    attempt = 0
    warning_placeholder = st.empty()  # Placeholder para mensajes de advertencia
    response_placeholder = st.empty()  # Placeholder para la respuesta del asistente
    run_id = None  # Inicializar el run_id

    while attempt < max_retries:
        try:
            print(f"Reintento {attempt + 1} de {max_retries}")  # Log en consola

            with response_placeholder.container():
                full_response = ''

                # Capturar el run_id solo del intento exitoso
                with collect_runs() as cb:
                    for chunk in chain.stream({"question": prompt, "history": history}, config):
                        if 'response' in chunk:
                            full_response += chunk['response']
                            response_placeholder.markdown(full_response)
                        else:
                            full_context = chunk['context']

                # Si la ejecuci√≥n fue exitosa, obtener el run_id
                if cb.traced_runs:
                    run_id = cb.traced_runs[0].id

                # Guardar respuesta del asistente en el historial
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response,
                    "run_id": run_id  # Guardamos el run_id correcto
                })

                warning_placeholder.empty()  # Limpiar el mensaje de espera
                return  # Salir exitosamente de la funci√≥n

        except botocore.exceptions.BotoCoreError as e:
            attempt += 1
            if attempt == 1:
                warning_placeholder.markdown("‚åõ Esperando generaci√≥n de respuesta...")
            print(f"Error en reintento {attempt}: {str(e)}")
            if attempt == max_retries:
                warning_placeholder.markdown("‚ö†Ô∏è No fue posible generar la respuesta.  Por favor, vuelve a introducir tu pregunta.")

        except Exception as e:
            attempt += 1
            if attempt == 1:
                warning_placeholder.markdown("‚åõ Esperando generaci√≥n de respuesta...")
            print(f"Error inesperado en reintento {attempt}: {str(e)}")
            if attempt == max_retries:
                warning_placeholder.markdown("‚ö†Ô∏è No fue posible generar la respuesta.  Por favor, vuelve a introducir tu pregunta.")


feedback_option = "faces"  # Puede cambiarse a "thumbs" si se requiere


# Modificar la funci√≥n main para usar la l√≥gica de reintento
def main():

    ## Esto es para mostrar mensaje inicial, de lo que puede o no hacer el chatbot
    #if "messages" not in st.session_state:
    #    clear_chat_history()

    st.markdown(
    """
    <style>
        /* Ocultar el men√∫ de los tres puntos */
        #MainMenu {
            visibility: hidden;
        }
        
        /* Ocultar el bot√≥n "Deploy" */
        .stAppDeployButton {
            visibility: hidden;
        }
    </style>
    """,
    unsafe_allow_html=True
)

    #### El session_id es lo que chat se le cargar√° al usuario
    query_params = st.query_params  # Para versiones recientes
    user_id =  query_params.get("user_id", "") #"langsmithfeedbackprueba@gmail.com"  #query_params.get("user_id", "")
    persona_id =  query_params.get("id_persona", "")#"101964"
    servidor = query_params.get("url_request","") #"C"  


    #st.write(user_id)
    if user_id:
        st.session_state.session_id =session_id = user_id  # Guardarlo en la sesi√≥n
        st.session_state.persona_id = persona_id  # Guardarlo en la sesi√≥n
        st.session_state.servidor = servidor

        st.success(f"Usuario: {st.session_state.session_id}")

        api_url = "https://compras135.ufm.edu/repositorio_procesos_api.php"

        if st.session_state.servidor == 'I':
            api_url = "https://intranet.ufm.edu/repositorio_procesos_api.php"


        # Par√°metros para el POST (form-data)
        payload = {
            "centroCostosPermisos": "1",
            "id_persona": st.session_state.persona_id  # Se env√≠a el ID de la persona desde la sesi√≥n
        }
         

        # Encabezados para la solicitud (form-data usa `x-www-form-urlencoded`)
        # Agregar m√°s encabezados, importante, sino se tiene User-Agent da forbidden
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }

        # Hacer el POST autom√°ticamente cuando hay un user_id
        with st.spinner("Obteniendo centros de costos..."):
            response = requests.post(api_url, data=payload, headers=headers)

  
        if response.status_code == 200:
            data = response.json()  # Convertir la respuesta en JSON

            # Guardar el JSON completo de los permisos en `st.session_state`
            st.session_state.centros_costos = data  

        else:
            st.error(f"‚ö†Ô∏è Acceso denegado: {response.status_code}")
            st.text(response.text)  # Mostrar el error en texto si lo hay
            st.stop()


    else:

        st.error("‚ö†Ô∏è Acceso denegado.")
        st.stop()  # Detiene la ejecuci√≥n de Streamlit

    
    history = DynamoDBChatMessageHistory(table_name=table_name, session_id=session_id)

    
     
    if "centros_costos" in st.session_state and st.session_state.centros_costos:
        # Filtrar solo los centros que tienen "ACTIVO": "Y"
        centros_activos = [item["NOMBRE_MOSTRAR"] for item in st.session_state.centros_costos if item["ACTIVO"] == "Y"]

        centros_texto = "\n".join([f"- {nombre}" for nombre in centros_activos]) if centros_activos else "No tienes √°reas disponibles."
    
    else:
        centros_texto = "No tienes √°reas disponibles."
        st.stop()


    descripcion_chatbot = (
    "\n"
    "Soy tu asistente sobre procesos de la UFM, ¬øEn qu√© puedo apoyarte?\n"
    "- Responder consultas sobre procesos espec√≠ficos, gui√°ndote paso a paso.\n"
    "- Mostrarte una lista de procesos relacionados y ayudarte a encontrar el proceso adecuado.\n"
    "- Proporcionarte enlaces directos a documentos y flujogramas relevantes.\n"
    "- Aclarar dudas y solicitar m√°s detalles para asegurar que obtengas la mejor respuesta posible.\n"
    "- Ofrecer informaci√≥n sobre tiempos estimados, participantes y aspectos clave de cada paso de un proceso.\n\n"
    "Mi misi√≥n es facilitarte el acceso a la informaci√≥n y guiarte a trav√©s de los procesos de la manera m√°s eficiente posible.\n\n"
    "---\n"
)  
    
    st.write(descripcion_chatbot)
    st.warning(f"Puedes consultar procesos de las siguientes √°reas:\n{centros_texto}")




    # Cargar mensajes de Dynamo DB y llenar el history
    if "messages" not in st.session_state:
        #clear_chat_history()

        st.session_state.messages = []
    
        # Cargar los mensajes guardados de dynamo DB
        stored_messages = history.messages  # Retrieve all stored messages
    
        # Llenar el estado de la sesion con los mensajes obtenidos, importante que se utilizan el rol user / assistant
        for msg in stored_messages:
            role = "user" if msg.__class__.__name__ == "HumanMessage" else "assistant"
            st.session_state.messages.append({"role": role, "content": msg.content})


    retriever = AmazonKnowledgeBasesRetriever(
    knowledge_base_id= "OWLYEEHPY5", ##Antiguo: "I9HQYMMI4A",
    retrieval_config=generar_configuracion_retriever_new()
    )




    SYSTEM_PROMPT = (f"""
                     
## Base de conocimientos (solo puedes responder con esta informaci√≥n):

{{context}}

- Debes **responder estrictamente con la informaci√≥n contenida en el contexto recuperado (`context`)**.
- **NO generes informaci√≥n que no est√© expl√≠citamente en `context`**.
- **NO inventes procesos, c√≥digos, ni nombres de procesos**. Si un usuario pregunta sobre un proceso que **no est√° en el contexto, responde que no hay informaci√≥n disponible o no tiene permisos para acceder a esta informaci√≥n**.
- Si `context` est√° vac√≠o o no contiene procesos relevantes, responde:  
  **"No se encontr√≥ informaci√≥n relevante sobre este proceso en la documentaci√≥n proporcionada o no tienes permisos para acceder a esta informaci√≥n"**
                     

## Instrucciones:

**Rol**: 
Adquiere el rol de un informador con conocimientos de metodolog√≠a de procesos y con gran oratoria para poder explicarlos de manera sencilla y clara. Estos procesos corresponden a la Universidad Francisco Marroqu√≠n de Guatemala, Panam√° y Madrid. Quiero que hagas preguntas al usuario para que mejore la forma en la que te solicita la informaci√≥n y no te centres en responder inmediatamente, si hay informaci√≥n que pueda estar en varias partes de la documentaci√≥n que te hemos agregado. No vas a buscar la informaci√≥n a internet, esto desvirtuar√≠a los procesos que hemos creado.

**Publico**: 
El publico objetivo es personal de la universidad, catedr√°ticos, profesores, personal administrativo de los departamentos y unidades acad√©micas y cualquier otra persona de car√°cter administrativo u otras √°reas. Es probable que no te den mucho detalle en su consulta, as√≠ que por favor centra la pregunta del usuario a√±adi√©ndole nuevas preguntas para mejorar el conocimiento de que quieren conseguir.

Siempre que recibas una consulta, debes hacer **preguntas de aclaraci√≥n** solicitando m√°s contexto y proporcionando una lista de los posibles procesos relacionados con la consulta, **ordenados por prioridad de mayor a menor relaci√≥n con el proceso (score), es decir que pueda encajar con un grado entre 0 y 1 de correlaci√≥n con la tem√°tica preguntada**. Usa la aproximaci√≥n para ello. La estructura de la respuesta inicial ser√° la siguiente:

1. **Preguntas de aclaraci√≥n:** Haz preguntas (por ejemplo si sabe el departamento al que pertenece el proceso, o preguntar al usuario que de m√°s detalles sobre lo qu√© quiere realizar ) para pedir m√°s detalles o confirmar el proceso espec√≠fico que el usuario desea obtener. 

2. **Lista de procesos relacionados:** Muestra una lista de procesos relacionados con la consulta **√∫nicamente** si est√°n presentes en el `context`. 
- **Solo menciona procesos que aparecen exactamente en `context`.**
- **NO debes generar procesos que no se encuentren en `context`.**
- **NO inventes c√≥digos de procesos ni hagas suposiciones sobre su existencia.**
- **NO asumas que existen otros procesos si no est√°n expl√≠citamente en `context`.**
- **Si el usuario pregunta por un proceso que no est√° en `context`, responde directamente que no tienes informaci√≥n y que el usuario no tiene permisos para acceder a esta informaci√≥n. NO intentes sugerir procesos similares de otras √°reas.**
- **Si el `context` no contiene procesos relacionados con la consulta del usuario, responde:**
  **"No se encontraron procesos relacionados con tu consulta en la documentaci√≥n proporcionada o no tienes permisos para acceder a esta informaci√≥n"**
- La lista debe estar **ordenada por prioridad** de mayor a menor, bas√°ndote en la relevancia de los procesos dentro del `context`.
- Usa el siguiente formato para cada proceso encontrado en `context`:
   - **Nombre del proceso (c√≥digo del proceso)**
   - Repite este formato para cada proceso relevante.

3. **Espera confirmaci√≥n:** (Mostrar un mensaje al usuario para que elija uno de los procesos mostrados, solicitarle que escriba el c√≥digo o nombre del proceso, adem√°s mencionar que si el proceso que busca no se encuentra puede ampliarse el listado de procesos relacionados)
   - Una vez que el usuario confirme qu√© proceso le interesa, procede a entregar la informaci√≥n detallada siguiendo los pasos descritos en la secci√≥n "Pasos Obligatorios" que aparece m√°s abajo.

4. **Si el usuario quiere cambiar de tema, preg√∫ntale si ha terminado con la consulta anterior, y as√≠ vuelve a repetir estos pasos tantas veces como el usuario necesite.**

## Pasos Obligatorios (Una vez confirmada la selecci√≥n):
Cuando el usuario confirme el proceso que desea conocer, sigue estrictamente los siguientes pasos (formatea la respuesta de manera clara y organizada):

1. **Identificaci√≥n del proceso:** Busca el proceso que te ha pedido el usuario y devuelve la informaci√≥n en formato tabla de la siguiente manera:
   - **Primera Columna:** C√≥digo del proceso mencionado.
   - **Segunda Columna:** Nombre del proceso mencionado.
   - **Tercera Columna:** Link al documento de pasos: Con un *hiperv√≠nculo* que diga **Ver documento de pasos** y el link-documento-pasos incrustado al mismo.
   - **Cuarta Columna:** Link al Flujograma : Con la misma estructura que el anterior con el link-flujograma, el *hiperv√≠nculo* con un texto que diga **Ver Flujograma**.

2. **Explicaci√≥n detallada:** Proporciona una explicaci√≥n lo m√°s detallada posible, que incluya:
   - **Objetivo del proceso.** [Descripci√≥n del objetivo o prop√≥sito del proceso].
   - **Pasos del proceso:** Aseg√∫rate de identificar **todos los pasos** desde el inicio hasta el final del proceso (por ejemplo, del paso 1 al paso 11).
        
     **Instrucciones para asegurar que no se omita ning√∫n paso:** 
        - **Verifica que todos los pasos consecutivos est√©n presentes**, desde el primero hasta el √∫ltimo (por ejemplo, si el proceso tiene 11 pasos, aseg√∫rate de que todos los pasos del 1 al 11 est√©n incluidos).
        - **Recorre todo el contenido disponible** y **combina la informaci√≥n fragmentada** si un paso est√° dividido en varias partes o aparece en diferentes secciones.
        - **No detengas la b√∫squeda hasta que identifiques todos los pasos declarados en la numeraci√≥n completa.** Si los pasos est√°n desordenados o fragmentados, organiza y presenta los pasos de manera secuencial (1, 2, 3,... 11).

        **Presenta cada paso siguiendo el formato a continuaci√≥n:**
        - 1. [Nombre del paso]:
            - **Descripci√≥n:** Explica el paso en detalle.
            - **Tiempos:** [Tiempo estimado para completar el paso].
            - **No negociables:** [Cosas que no se pueden omitir o cambiar].
            - **Participantes:** [Personas o √°reas involucradas].
        - 2. [Nombre del paso]:
           - **Descripci√≥n:** Explica el paso en detalle.
           - **Tiempos:** [Tiempo estimado para completar el paso].
           - **No negociables:** [Cosas que no se pueden omitir o cambiar].
           - **Participantes:** [Personas o √°reas involucradas].

        - Repite este formato para cada paso. Aseg√∫rate de incluir todos los fragmentos relacionados con el proceso y de que cada subelemento (como tiempos, no negociables, participantes) est√© separado con saltos de l√≠nea (`\n`) para mantener la estructura clara y organizada.
        
        
    - **Confirma con el usuario si pudo resolver su consulta**.


## Reglas para Fragmentos de Informaci√≥n:
- Si el proceso est√° dividido en varios fragmentos o "chunks", debes combinar toda la informaci√≥n relacionada con el c√≥digo del proceso antes de generar la respuesta.
- Usa el **c√≥digo del proceso** para identificar y concatenar todos los fragmentos que pertenezcan al mismo proceso, proporcionando una explicaci√≥n completa sin omitir ning√∫n paso, tiempo, no negociable o participante, incluso si est√°n en diferentes *chunks* o fragmentos.

## Listado Completo por Unidad ##
Si el usuario solicita ver un listado completo de los procesos de una unidad, responde con el siguiente formato:

1. **Listado Completo de Procesos:**
   - Presenta todos los procesos disponibles en `context` de la unidad solicitada.
   - Utiliza el siguiente formato para cada proceso: (Repite este formato para cada proceso):
       1. Nombre del proceso (C√≥digo del proceso)
       
       **Ejemplo:**
        1. UFM-CODIGOAREA-001 - Proceso A
        2. UFM-CODIGOAREA-002 - Proceso B 
        3. UFM-CODIGOAREA-003 - Proceso C 

   **Nota:** Si no se encuentran procesos para la unidad solicitada, responde: 'No se encontraron procesos para la unidad o no se tiene permiso para acceder a esta informaci√≥n'

## Manejo de Consultas sin Informaci√≥n Relevante:
- **Si el usuario pregunta por un proceso que no est√° en `context`, responde sin inventar informaci√≥n.**
- **NO generes respuestas basadas en conocimiento previo o inferencias si la informaci√≥n no est√° en `context`.**
- **Si `context` est√° vac√≠o o no contiene procesos relevantes para la consulta del usuario, responde con:**
  **"No se encontr√≥ informaci√≥n relevante sobre este proceso en la documentaci√≥n proporcionada o no tienes permisos para acceder a esta informaci√≥n."**
- **NO intentes reformular la consulta ni sugerir procesos fuera del contexto recuperado.**

## Manejo de Respuestas Cortas 
- Si la consulta solo requiere un enlace o un dato espec√≠fico (nombre o c√≥digo de proceso), proporciona √∫nicamente esa informaci√≥n sin desglosar todos los pasos.

    """
    )


    chain_with_history = create_chain_with_history(retriever,SYSTEM_PROMPT)


    # Mostrar historial de conversaci√≥n
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            st.write(message["content"])

            # Mostrar feedback solo para respuestas del asistente
            if message["role"] == "assistant" and "run_id" in message and message["run_id"]:
                feedback_result = streamlit_feedback(
                    feedback_type=feedback_option,
                    optional_text_label="[Opcional] Comentarios adicionales",
                    key=f"feedback_{message['run_id']}",
                )

                # Guardar feedback en session_state y LangSmith
                if feedback_result and "score" in feedback_result:
                    message["feedback"] = feedback_result["score"]

                    # Mapeo de puntuaciones
                    score_mappings = {
                        "thumbs": {"üëç": 1, "üëé": 0},
                        "faces": {"üòÄ": 1, "üôÇ": 0.75, "üòê": 0.5, "üôÅ": 0.25, "üòû": 0},
                    }
                    score = score_mappings[feedback_option].get(feedback_result["score"])

                    if score is not None:
                        try:
                            feedback_type_str = f"{feedback_option} {feedback_result['score']}"
                            feedback_record = client.create_feedback(
                                message["run_id"],
                                feedback_type_str,
                                score=score,
                                comment=feedback_result.get("text"),
                            )
                            message["feedback_id"] = str(feedback_record.id)
                            st.success("‚úÖ Feedback registrado correctamente.")
                        except Exception as e:
                            st.error(f"‚ùå Error al enviar feedback: {e}")

    # Capturar input del usuario
    if prompt := st.chat_input("Escribe tu mensaje aqu√≠..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        config = {"configurable": {"session_id": session_id}}

        # Capturar el run_id con collect_runs()
        #with collect_runs() as cb:
        invoke_with_retriesnew(chain_with_history, prompt, st.session_state.messages, config)
########
            # Guardar el run_id generado en la respuesta del asistente
       #     if cb.traced_runs:
       #         run_id = cb.traced_runs[0].id
       #     else:
       #         run_id = None

        # Obtener la √∫ltima respuesta del asistente y a√±adir el run_id
       # if st.session_state.messages and st.session_state.messages[-1]["role"] == "assistant":
       #     st.session_state.messages[-1]["run_id"] = run_id




if __name__ == "__main__":
    main()




